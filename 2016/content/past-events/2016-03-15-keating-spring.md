# Spring 2016 conference review I

## Mark Keating

This is a general impression of the recent FLOSS UK event that I attended. I should point out that I do act as a member of the organising team and the FLOSS UK Council but this is a personal review of the experience not an official account.

The 2016 FLOSS UK Spring DevOps Conference was held in London at the historic Mary Ward House between 15th-17th March. This year’s conference could have held the subtitle of 'configuration management and communities’ as there was a strong undercurrent of both. The spring conference is special to the members of FLOSS UK as it is the oldest DevOps event of this type in the UK, by which we mean dedicated more to systems than programming alone -- though that is not a strongly defined prerequisite.

### Changesmaybe

In the build-up to the event the FLOSS UK Council undertook a couple of new moves as they continue a transition from supporting just Open Source Software to supporting openness across a number of disciplines such as data, hardware, rights and standards. This is reflected in their new tagline Open Technology and the notion that FLOSS has evolved to mean Free and Libre Open Source Systems to reflect the changing world.

The FLOSS UK Council also took the decision this year to have an ‘anonymous’ submissions system. This wasn’t advertised as the Council wanted to test the process without being rigidly defined. It would also allow for the methodology to be tested for issues. Submissions were anonymised by a non-voting member and presented to the Council as simple abstracts. The Council weighted each submission with a score which was tallied by the non-voting member. They then ranked the submissions by order of score which gave them a selection group, from that there was a consensus as to which talks would make good plenary/keynote speeches.

There is a feeling that this has resulted in a broader talk base, especially in the plenary talks, in comparison to previous years. There is some discussion as to how this is evolved for next year. One change will be that the anonymous submissions will be used again and will be advertised as such and the Council hopes this will further encourage a variety of submissions.

### Mary Ward House

The historic Mary Ward House in London’s central Bloomsbury district was financed by the wealthy philanthropist Passmore Edwards. It is named for Mary Ward, the novelist and social reformer, who was the inspiration behind the endeavour to provide a centre of training, care and entertainment for the less fortunate in society. One of the most stunning features of the building is the Dickens’ Library which is where the sponsors had tables and breaks took place.

Mary Ward House is one of the more unusual, and dramatic, conference venues in the country and as such has a small wealth of peculiarities. The historic nature of the building gave a wonderful taste to the atmosphere of the conference but I feel it also made for a number of issues that in the end overwhelmed the positive.

There are a number of elements that I think the event owners need to address if they wish to hold conferences of any size here in the future. If we were a limited event, say one day, in a single room then the venue is a pleasant meeting space, but for a three day conference there were a number of small issues that marred the overall event slightly. One of the largest issues was the echoing rooms with high noise transportation from room to room.

My general impression is that the venue mostly caters to groups seeking a small space for a half or single day event. There seems to be no conception of standard needed for a longer conference. This may be down to the event having restructure and refurbishment work, or just inexperience. The staff themselves were generally very pleasant if a little uninformed of all that was needed to make a conference work.

### The Presentations

FLOSS UK Spring is a three day event. The main part of the conference takes place on two days but there is a workshops day which precedes these. This year we had two primary workshops, selected from five available by the attendees. Kimball Johnson did a whole day of Chef training and Paul Waring did a half day on Ansible with Git.

The workshops seemed to be well received by the participants with a good mix of discussion on the two different systems. Paul had a larger attendance list, which is generally the case as half day events are always more popular with an audience used to late starts.

The main part of the conference was two days with two tracks of talks. The talks themselves were mostly 50 minutes long with a few 30 minute talks to allow lunchtime to be slotted into the schedule.

A new element for this year was the inclusion of BoF (Birds of Feather) sessions into the main schedule. These are aimed at structured discussion on particular topics, a formalised version of the hallway and social tracks. They are run at a number of conferences as they allow casual conversation but in a structured environment on a dedicated subject. They went well and plans are in place to run them again next year.

As always some of the best talks were the lightning talks. These are always fun and frivolous. I chaired the session this year and I kept everyone on their toes with a strict 5-minute time slot to present. My particular favourites were Paul Waring and Matt S. Trout. Though I agree with the audience who selected the talk on ‘River, Home and Mother’ automation and observation.

The plenary, or keynote, talks at the conference this year were given by Mandi Walls and Lameck Amugongo. Mandi gave the opening talk called ‘Always be Learning’ aimed as a lesson about being in an open source life. Lameck Amugongo closed the conference with ‘Data Revolution Catalyst’ when he spoke about the empowerment of using open data in emerging economies, in particular how in Namibia they are using ‘open data’ to help communities gain access and enablement. I will cover the talks that I attended in a little more detail below

### Sponsors

It is always nice to see sponsors at any event and particularly when they bring along such great swag for us to play with. This year was a bonus with awesome notebooks, toys, pens, t-shirts, flash drives and power packs from the wonderful people at Chef, Suse, Google and Eligo.

A special thanks to Roger and SUSE who have been with FLOSS UK for a long time and constantly give strong support and present talks. Also to Rick at Eligo who sponsored and gave a talk on ‘Developing Your Brand’.

### The Presentations

Disclaimer: The following are a description of a presentation with some dissection of what was discussed. It is not a verbatim account and will contain personal impressions and interpretation. The content therefore does not reflect the quality of the original presentation and should be considered a review and personal opinion.

### Always Be Learning: Mandi Walls

Mandi is a software developer currently working for Chef in London, the talk wasn’t about Chef though, it was about the Open Source world and our part within. Mandi started with a discussion of an early large HP conference that she attended. The event had a number of communities gathered together to discuss a certain topic, however they were still segregated, this is an attitude that still persists today. We might have shared features but we remain tribal about our software choices.

The world itself has changed, when you meet people today there is a sea change from the days of massive, expensive, and singular control systems to smaller, cheaper, multiple systems that are deployed and made to interact with each other via open APIs.

Part of this change has been at the behest of the adoption to Open Source, OS software is now a dominant player in many larger systems, corporations and projects. It has taken over from proprietary systems. This gives rise to the idea that:

>technology becomes more varied when not constrained by controls

There are a number of issues. One of the ones that aggrieves Mandi is the larger enterprise customers who use open source software in ‘largely an opportunistic fashion’. This is where they will build large systems upon open source but fail to engage with contribution to code or communities that created the software.

Open source sometimes seems to sneak into corporate environments, this might be because of: advocacy by a particular programmer; by being the only acceptable solution without creating an in-house solution; or having features and abilities not found in proprietary systems.

>Organisational change is hard, some of them seem to be doom-laden, heading inexorably, Titanic-like, towards a disaster. They are risk-averse and therefore change-averse.

It is often too hard to change the mindset of an entire organisation, even with a great deal of advocacy or solution solving, therefore the goal should be shifted towards the individual.

>Changing orgs is hard but people-change is easier.

This sea of change has started to enter the corporate world. It is now impossible to find careers that last a lifetime in a single organisation, or organisations who look further than a decade in terms of their existence. Organisations, like the software platforms they depend upon, seem to have a shorter and more evolutionary lifespan. Corporations, systems and programming languages seem to fall in and out of fashion without and real reasons or need.

There is a growing pace in this state of change so that it is increasingly hard to keep abreast of changes to software, to innovation and to the new.

This has reached a crunch with developers, innovators and technical employees where Enterprise has exported the risk of learning and acquisition of new knowledge onto the individual, removing it from the corporate structure. This pressure unfairly benefits those who are younger or unattached and places greater responsibility on individuals to:

* Gain experience in multiple existing technologies.
* Be pliable and flexible to new learning.

Part of an individual's response to this, in order to keep some sense of progression, must be to build ‘aspiration models’ into their daily life. There is a need to ‘be naturally curious’. Programming attracts people who are naturally curious, who want to see how something works, and this behaviour should be encouraged. You may need to ‘make yourself curious’ about a topic by using ‘curious language’ in order to relate or describe it.

You must learn to embrace your vulnerabilities, to see them not as a weakness but as an area to be aspirational about. You must always approach anything as a learning experience, this is especially true if you think you know it, try to approach it as if you do not understand, use the language associated with learning.

>New is scary (and exciting) and that is a great challenge

In this learning environment where you may have greater knowledge or experience it is important to be wary of creating a negative environment. This is especially helpful to those who do not yet know but are trying to learn. So many people have turned away from a particular solution or language because of the toxicity encountered with their first innocent questions.

Learn to be self-aware. Be wary of how attitudes can force behaviours. Anyone who has knowledge and experience has a position of power and therefore a responsibility. Your opinion, your choices, will create an environment that may be opposite to what you may want to achieve.

Mandi finished with some basic principles:

* Learn cool stuff.
* Hang out with cool people who build stuff.
* Build your own skills.
* Come to know about the cool things.
* Always look out for an opportunity and ways to learn.

### When Applications Make Promises: Thom May

Thom is an applications developer with Chef, his talk was focussed on how Chef works in regards to Promise Theory.

> Promise Theory, in the context of information science, is a model of voluntary cooperation between individual, autonomous actors or agents who publish their intentions to one another in the form of promises. A promise is a declaration of intent whose purpose is to increase the recipient's certainty about a claim of past, present or future behaviour. For a promise to increase certainty, the recipient needs to trust the promiser, but trust can also be built on the verification that previous promises have been kept, thus trust plays a symbiotic relationship with promises. Each agent assesses its belief in the promise's outcome or intent. Thus Promise Theory is about the relativity of autonomous agents.[^promise-theory]

[^promise-theory]: [https://en.wikipedia.org/wiki/Promise_theory](https://en.wikipedia.org/wiki/Promise_theory)

The way that Promise Theory is implemented in Chef underpins much of how it works as a configuration management system, the belief in Chef is that systems work well with Promise Theory.

In a configuration management promise we have the Promise, the Changes and the Outcome. In Chef this turns into:

* Node is a promise
* Node declares what it will be
* Node reports if it was successful and why (changes) that led to that success

In this way the whole of the configuration management promise is ‘data driven’

> On millions of nodes is it implausible to directly control each one of them. Their has to be a level of stratification, but in massive systems there has to be the expectation of automation.

Chef uses Orchestration is synchronous, it uses a direct connection not APIs, this is an overhead and limits the number of simultaneous connections, so this needs scheduling. This is a complex system and relies on the use of central logic to control, the install code for everything is inside the orchestrator.

Chef also uses Actors which are the instances and are synonymous in Chef with applications. An actor can receive a message, send a message, spawn new actors, change itself - which is to change the system and state by sending a message.

Orchestration allows the actors to be manipulated which is considered to be Choreography. In choreography there are a number of elements, a promise is made, agents are the ones to make a promise, they can perform actions, this delivers dynamic relationships between nodes.

The use of Promise Theory to make a contract that the system performs using applications as actors that perform a  specific routine (choreography) that is centrally directed (orchestration) and controlled makes for a very powerful system. It occurs to me that by using source control via repository and management it would serve as a change control system for the systems as well as a configuration management. The fact that Thom was able to convey all of this understanding and information in such a clear manner is mark of how great his presentation was. It certainly makes for a good introduction to the conceptual working of Chef.

### Configuration Management with Ansible and Git: Paul Waring

The conference had a strong feel of configuration management to it and the second talk on the subject I attended was from Paul Waring about using Ansible. A section of Paul’s talk was a little confusing to me as I lost the thread in the second half of his presentation. This was less to do with Paul’s presentation style and more my inexperience with the subject and sitting too far to the back of the room so that I couldn’t follow the slides (again this wasn’t Paul’s fault the room had very large windows and no blinds).

To begin Paul discussed why there was a need for Configuration Management, the situation was that packaging and installation was a manual process. This was boring for the developer to repeatedly perform; time consuming, and so therefore costly; prone to mistakes, mostly due to the first two reasons. Ansible seeks to solve this issue, in much the same was as Chef, a major difference is that Ansible calls orchestration a ‘playbook’.

Ansible is fully open source software and protected under the GPL3, it is available on Github. Ansible has been incorporated into the RedHat Linux distributions when they acquired the parent company that supports its development.

Ansible has a minimal set of dependencies and these are found in most Linux distributions. It also works cross environment to the other popular operating systems. It has become known as working well in both smaller environments as well as large server farms. It goes without saying that Ansible has probably the best integration with RedHat which is a favoured distribution in corporate environments. If you have a larger configuration management, change management or process management plan then Ansible can be a player.

Ansible is written in an INI format which is based on the principle that it is easy to configure and to write. In this manner if the requirement is for a small rollout of a staging and deployment server then Ansible works well for easy replication. The requirement is that the person configuring Ansible must be in a User Trust Status or logged in as Root. You can specify keys and host names if required.

Ansible uses a distributed environment for adding extra features which are created as modules. To configure Ansible you write a set of instructions in the playbook.

Ansible works well with Git for a central repository, you can use other management systems like CVS and SVN. The advantage of using Git is the powerful way in which it uses rollback, undo and rebase, the good view history and the cheap method for utilising branches. You can also use Git hooks to perform actions in the workflow both as pre- or post-commit. This would allow you to perform tasks such as running unit tests, checking syntax and checking dependency structure.

Ansible seems like a lighter configuration system than Chef or Puppet and using Git as an extra layer for working with this adds to its value. Much like Thom’s talk Paul was able to convey how Ansible works it was just unfortunate to me that I couldn’t read the examples he gave on his slides.

### Icinga - Middle of your Toolchain: Bernd Erk

Bernd is a core developer on the Icinga project and has returned to the Spring Conference on a yearly basis to tell us about the latest developments. In the last few years the project has gone from strength to strength and the Icinga Version 2 carries that torch far forward. Icinga was originally a fork of Nagios with the intention of making it more accessible.

Icinga 2 takes this to a new level by re-writing the underlying system, it now has no share with its original parent and is only forwards compatible (you must migrate) from Icinga 1 to 2. Icinga 2 does however still allow you to use anything from the Nagios plugins. Bernd made a passionate plea, well passionate for Bernd, at the event that if you are using Icinga you make it known to the development team, at this moment they have no way of tracking who is using the software and which version and they would like to know.

The focus on Icinga 2 has been the need to make it scalable and extensible. At the same time they have launched a web-based Icinga Web 2 that needs less configuration. Icinga 2 has a new way to configure which is not compatible with either Nagios or Icinga 1, it uses an enhanced configuration language. Icinga 2 has no enterprise edition, they have one consolidated edition whose entire source is open.

Icinga 2 also uses Icinga Director which is a configuration management for Icinga and user commit management. As they roll out Icinga 2.4 they will also roll out new modules to support it including the Graphite, Grafana and Elastic Search.

Bernd spent a good 15 minutes giving us a live demonstration of the Icinga Web 2 system and it looked extremely promising. The use of more modern graphing systems, of exporting the metrics to other systems and allowing configuration management are all mature decisions of a project that gets stronger with each new release.

### Developing Your Brand: Rick Deller

Rick Deller is an Open Source recruitment specialist at Eligo. Before you start to instantly make assumptions he is probably one of the few really good recruiters as he does care about the industry and the communities. Rick, and Eligo, are sponsors of a number of community events and Rick is always on hand to help out. One of his ways of contributing is in talks. Last year Rick spoke about how to write a good CV for your next job and this year he returned with the ideas of how to develop yourself as a brand to make your career stronger.

Rick wanted to make sure that we all understood that in a world that is quickly becoming dominated by your online presence the notion of public and private are becoming blurred. So part of this is to control your image and project yourself correctly, that is making your brand. When someone looks at you online you have a short period of time in which to make the best impression. There are a number of things you should do:

 * Only show core skills
 * Show projects
 * Core achievements
 * Blogs
 * Social profile
 * Make sure your Linked-In page is up to date
 * Peer recommendations
 * Be careful of your online attitude
 * Make sure you are visible in your community.

There has to be another side of this equation. If you are a company and you are seeking to employ the right people for your positions you can also do a little more to make yourself more attractive to the best applicants. So you should be aware of:

 * How you connect, speak to people in their language
 * Representation of your business
 * Your reputation in the marketplace
 * What technologies you use
 * The management structure
 * What do your ex-employees say?
 * What is the company progression?
 * What are the training and skills?

As always Rick spoke with a quiet authority on this subject. I have since discussed with Rick his two talks and know that he is keen to return in 2017 with the next stage. He is also developing a workshop to help people practice skills of choosing the right company, presenting themselves and getting a good career path.

### Is Configuration Management Still Relevant in a Containerised World: Stephen Grier

Stephen Grier works at the University of Central London where he mostly focuses on what he terms as ‘web technologies’ and utilises Puppet for configuration management. At the start of the talk he posed the following question:

>What does Configuration Management actually give us?

To Stephen it provides:

 * Automation
 * Reproducibility
 * Reliability
 * Infrastructure as code
 * Convergence towards a desired state

We should look at containerisation. To Stephen the idea is gaining popularity now but it is not a new idea. In the 1970s we were introduced to a concept of it in the CHROOT daemon, spin forward a couple of decades and FreeBSD has the first container project with the introduction of Jails. This is probably the first proper usage of containers as we have come to see them today.

Since the field was only occupied by Jails for a decade and then has grown quickly there is a sense of a new land. As such there are few similarities to existing ideas and virtually no standards for containers. However due to popularity Docker has now emerged as a de facto, or proto-, standard.

Steven gave a list of what he sees as the reasons for using containers:

 * Process isolation
 * Security - processes can’t interfere with each other
 * Resource limitation
 * Portability
 * Self-contained with all their own dependencies
 * Fast to deploy
 * It is a unit of deployment
 * Non-repudiation

There is a sense that containers can allow you to forget about versions, dependencies as when the container is created it is properly baked. Then it is just a matter of controlling it. You can even add levels of security as to who can bake a container by using a notary framework. It is also wise to use just one process per container, containers do allow you to use more than one process (after a fashion) but this is an ill-advised behaviour.

Since the containers are baked and a known unit we suddenly have a measurement for our deployment systems, there is a ‘unit of deployment’ that is a container and its contents. The nature of containers means that we do not usually need to config manage them, but then we will have to build the container images manually. This seems like a backwards step in automation. With Docker there are Docker-files to build the Docker images to make the manual process creation easier but it still feels retrogressive in regards to configuration management.

Then we come to the issue of management and control of an underlying system. Docker may give application, or more correctly process, containerisation but you may still need to manage a number of underlying kernels across many servers or VMs.

Stephen recommends the use of a system such as CareOS. It would seem that although the top level system can be containerised there is still a place for configuration management of the underlying systems. This is where Puppet has a place as management of Docker using Puppet is made easy with a gathr-docker module.

The advantage of containerisation is that you can define applications with code. You can now move from just having the infrastructure as code to having the application layer as code.

>To bake an image you need a recipe

Stephen used the above (Chef-aimed) pun to indicate that although containers are a step forward you still need the power of configuration management but it is in a much reduced manner. Configuration Management tools make a powerful way to declare the state of an image with values, resources and packages. So we should run configuration management tools  to make local file baking automated and share this across our systems.

>Convergence is less important, declaring desired state is more important.

So configuration management is still relevant, but in a greatly reduced capacity.

### Architecture Automation: Matt S. Trout

The talk from Matt was first presented two years ago but Matt recently updated it and delivered it for us at FLOSS Spring. The talk is still relevant as it is a series of steps for good automation practices when you come to automate a system not a discussion of a recent piece of software.

>When you’re up to your ass in alligators it’s hard to remember you were supposed to be draining the swamp

Most of the talk was delivered as an homage, and dedicated, to a man called Wild Bill Walton, originator of the above quote, who was a man who in Matt’s words:

>Managed to deliver a metaphor that both managers and technical people could understand…[and was a]...master of the folksy metaphor

The premise of this talk is that you may have to automate a server in your existing company, or a new company, or an acquisition, where there has been no accurate record of what has been placed on it. Even if you have a good development environment, use virtual machines and containerisation for segregation of services, it is still possible that you may have to do forensic analysis of someone else’s work.

It is also possible that you are working as a start-up and don’t have the option to do things ‘the right way’ in the first instance:

 * Build customers first
 * Build technical debt - this is inevitable and is not ‘bad’
 * Refactor when you have the time and funds
 * Don’t 2nd System yourself up-front

The ideal situation is to be in profit and at a period of relative stability. You may have rotated through some developers and iterations of codebase and have systems that now are starting to be the blockage in your evolution. Now we:

 * Figure out what systems you have
 * Figure out what services are where
 * What’s installed
 * What’s currently running

To do this we start by asking the Operating System and then work out the custom code. One thing to be wary of is if someone did development on the production machine that isn’t replicated anywhere else.

>Grep everything for IP addresses...Firewalls are not just for security, they control what connects and force transparency

It is essential that you use configuration management even on small systems, some automation is better than no automation. The recommendation that Matt gave is that Pull-Based systems are preferred.

>Make sure it is Predictable, Repeatable, Stupid, Understandable - don’t try to be clever, systems should not be smarter than you. Don’t trust DNS TTL to be honoured by other systems: keep it simple, keep it stupid, keep it one Alligator at a time

### Open Source: A Job and an Adventure: Dawn Foster

Dawn has spent a number of years working across the broad spectrum of software development organisations in the United States and beyond, she gave a very personal account of how to have an open source career and life. Her earlier career was working with manufacturers, since they do not see software as anything but a sunk cost, it isn’t their income they are averse to spending large sums. It is for this reason that there is a generous uptake of open source in the manufacturing industries.

This is where Dawn first encountered open source and she quickly became fascinated by how the communities worked and in particular how they came to evolve such complex software projects.

Dawn covered three main areas in her talk as well as highlighting many of the people she had worked with across various communities. She spoke on:

 * Why have a career in Open Source?
 * How to make it happen.
 * Time Management - and how to avoid burnout.

This very personal lists of reasons has echoes for me personally as it could have been a list I wrote myself. Dawn managed to summarise quite distinctly the reasons to be involved with communities. Splitting them into the three sections made them easier to absorb and relate to.

Why?

 * Meet people from all over the world.
 * Meet them in different groups - the same people often belong to more than one community.
 * Travel - attend conferences and work with different companies of people.
 * Open Source feels like you can talk more freely and companies seem to enjoy being known for their use of free software - a badge of pride.
 * Open Source gives you more personal visibility.
 * Open Source shows your talents and work.
 * You get to have fun even with those people who are your competitors.

How:

 * Start a new project - it should always be something that you need, not what you think others will want.
 * Open Source projects need people with lots of skills.
 * Work for an open source company.
 * Bring open source software into your current job.
 * Write and speak about open source.
 * Try consulting on the open source skills you have.
 * Documentation - the best route into any new project and the most common forst step.
 * Be Nice.

Time Management:

 * Prioritise.
 * Document processes and procedures.
 * Take time off for hobbies or a vacation.
 * Do something fun each day.

Dawn’s talk was sensible, empathetic and thoughtful. It was clear that she had found a good balance in both her life, her career and her open source participation which was healthy for her.

### Data Revolution Catalyst: Lameck Amugongo

Lameck, who currently works in Germany, is originally from Namibia and still participates heavily in the open source and open data movements in his home country. Namibia is on the cusp of a new era in how it deals with open access with strong movements from their government towards empowering people using open standards.

>We need data for everything

The idea of using open data, big data (government and large organisation records and readings) has been enshrined as a right by the United Nations who have called for 17 data codes to protect and distribute data to people.

>Open data will help us to build sustainable tools and a sustainable future

In 2013 McKinsey and Company estimated the value of big data to be in excess of three trillion dollars, in 2015 that had raised to five trillion. This estimate is considered to be a conservative rationale. We are still in a fledgling stage with the use of big/open data, there are a number of needs that must be addressed as we move towards a more accessible future:

 * Research into the current levels of availability and usage.
 * Measuring via access, applications or solutions that use the data.
 * Raw data available to all via on- and off-line access.

In emerging economies the best usage of this data is via the mobile networks. New dominant countries have stronger telecommunications infrastructure via mobile and satellite than traditional wire or fibre networks.

Why Open Data?

 * Social functions.
 * Economic freedom.
 * Cultural need and change.
 * Civic engagement.
 * Decision making, and electoral services.
 * Transparency of activity and accountability of usage.

This data revolution is not free but the people have already been paying their governments and organisations to collect it for decades via the existing mechanisms of taxation and civil records. There has also been massive government investment into environmental monitoring and scientific research producing petabytes of data for analysis and statistical modelling.

This data is closely tied to the phenomena often described as ‘the internet of things’ (IoT) - certainly the availability of cheap hardware and sensors, coupled with connected systems will drive a large wave of data collection. This could be a way of resolving the major challenges the world is facing, if it is available and understandable.

#### The Issues

There are some major problems in this data-driven world. There is a huge discrepancy caused by digital divides, this isn’t just the access to digital material but the competence in using or understanding it. Open data can give us a greater wealth of understanding and it can enrich the lives of everyone who has access to it. As long as they have access and can use it.

#### Open Data Innovation Hackathon

The Open Data Innovation Hackathon (ODIH) is an initiative created in Namibia and backed by the government to create applications that make use of the ideas behind Open Data. It has been held for the last 2 years and has created a number of innovative applications that are being developed for free distribution.

 * GoToVote - Kenya - Helping people to register and to vote in elections.
 * High Fives - Tracking the development of the African Continent.
 * Fix My City - Reporting on local infrastructure issues - allows citizens to report directly to local authorities.
 * Smart Meters - allows you to monitor and preserve water usage.
 * eMERGE - help with emergency services notifications, report an issue and a location.
 * Food Bank - trying to eradicate poverty by reporting food levels, access and need.
 * eHealth - Pay for health services.
 * CoW Bus Service - increase the mobility of citizens and access to transport.

Lameck proposes that the way forwards is the creation of more APIs to access and share the data.

 >Big data is velocity, volume and variety, it is about dynamic scaling and performance. To use it we need flexible schemas, semi-structured data sources and personalisation of data to make it relevant to each individual so that they are engaged. It should enable us to process, understand, help and serve us in our environment. We need real time data and that includes operational data for access and consumption in real time.

The conclusion is that data has become an essential component of a modern writable society. To support this we need to create flexible bureaucracy that better suits the pace of technological change and digital freedoms.

> It is our data, let’s liberate it.

### One Step Beyond

Next Year’s Spring Conference was announced at the end of the event. As usual the conference will move to a new city, this time it will be in Manchester, once again in March. The venue has yet to be announced though it is in negotiation/discussion, there will be workshops.

On the way to DevOps 2017 there are a number of FLOSS events this year. There will be Barcamps, in London, Birmingham and possibly Manchester.

DevOps is now the buzzword for what would have been programmatic system administration. The field seems new but is in fact mature and this is reflected in the quality, breadth and depth of the presentations and workshops at the FLOSS conference. The videos of the talks will be available on the FLOSS YouTube channel[^flossuk-youtube] in the coming weeks.

[^flossuk-youtube]: [http://www.flossuk.org/youtube](http://www.flossuk.org/youtube)
